---
title: "Fatemeh Rajaeian"
format: html
---

```{python}
import pandas as pd
from nltk.tokenize import word_tokenize
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
```
```{python}
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```

```{python}
df = pd.read_csv("E:\\Data Science for Society and Business\\2 Semester\\Text analysis\\JobDataAnalysis.csv")

```
```{python}
print(df.head())
```
```{python}
df.shape
```

```{python}
columns_to_drop = ['web-scraper-start-url', 'DataAnalyst-href', 'JobTitle']
df = df.drop(columns=columns_to_drop)
```
```{python}
# Function to preprocess text
def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()
    
    # Remove punctuation and special characters
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    
    # Tokenize text
    tokens = word_tokenize(text)
    
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    
    # Join tokens back into a single string
    text = ' '.join(tokens)
    
    return text

# Preprocess job descriptions
df['clean_description'] = df['JobDescription'].apply(preprocess_text)

# Display the first few rows with cleaned descriptions
print(df[['DataAnalyst', 'clean_description']].head())
```

```{python}
# Exploratory Data Analysis (EDA)
all_descriptions = ' '.join(df['clean_description'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_descriptions)

# Plot word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Job Descriptions')
plt.show()

# Calculate word frequency distribution
word_freq = pd.Series(all_descriptions.split()).value_counts()

# Plot frequency distribution (top 20 words)
plt.figure(figsize=(12, 6))
sns.barplot(x=word_freq[:20].values, y=word_freq[:20].index, palette='viridis')
plt.title('Top 20 Most Common Words in Job Descriptions')
plt.xlabel('Frequency')
plt.ylabel('Word')
plt.show()
```
```{python}
# TF-IDF (Term Frequency-Inverse Document Frequency) 
# Concatenate all preprocessed job descriptions into a list
preprocessed_descriptions = df['clean_description'].tolist()

# Initialize TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the preprocessed job descriptions
tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_descriptions)

# Get feature names (terms)
feature_names = tfidf_vectorizer.get_feature_names_out()

# Get TF-IDF scores for each term in the first job description
job_description_index = 0
tfidf_scores = tfidf_matrix[job_description_index]

# Sort TF-IDF scores in descending order
sorted_indices = tfidf_scores.toarray().argsort()[0][::-1]

# Print top N important words or phrases with their TF-IDF scores
top_n = 10
for i in range(top_n):
    term_index = sorted_indices[i]
    term = feature_names[term_index]
    tfidf_score = tfidf_scores[0, term_index]
    print(f"{term}: {tfidf_score}")

```
```{python}
# Topic Modeling
# Concatenate all preprocessed job descriptions into a list
preprocessed_descriptions = df['clean_description'].tolist()

# Vectorize text data using TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=1000, max_df=0.95, min_df=2, stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_descriptions)

# Apply Latent Dirichlet Allocation (LDA)
lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
lda_topics = lda_model.fit_transform(tfidf_matrix)

# Apply Non-Negative Matrix Factorization (NMF)
nmf_model = NMF(n_components=5, random_state=42)
nmf_topics = nmf_model.fit_transform(tfidf_matrix)

# Display top words for each topic
def display_topics(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic {topic_idx}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))
        print()

print("LDA Topics:")
display_topics(lda_model, tfidf_vectorizer.get_feature_names_out(), n_top_words=10)

print("NMF Topics:")
display_topics(nmf_model, tfidf_vectorizer.get_feature_names_out(), n_top_words=10)
```